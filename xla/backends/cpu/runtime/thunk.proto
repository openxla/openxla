/* Copyright 2022 The OpenXLA Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

syntax = "proto3";

package xla.cpu;

import "xla/backends/cpu/runtime/collective_thunk.proto";
import "xla/service/hlo.proto";
import "xla/xla_data.proto";

option cc_enable_arenas = true;

message CallThunkProto {
  ThunkSequenceProto called_sequence = 1;
}

message ConditionalThunkProto {
  repeated ThunkSequenceProto branch_sequences = 1;
  BufferAllocationSliceProto branch_index_buffer = 2;
}

message ConvolutionThunkProto {
  message Options {
    bool multi_threaded = 1;
    bool use_acl = 2;
  }
  ConvolutionDimensionNumbers dimension_numbers = 1;
  Window window = 2;
  int64 feature_group_count = 3;
  ShapeBufferAllocationSliceProto input_buffer_shape = 4;
  ShapeBufferAllocationSliceProto kernel_buffer_shape = 5;
  ShapeBufferAllocationSliceProto output_buffer_shape = 6;
  Options options = 7;
}

message SortThunkProto {
  enum SortDirection {
    UNKNOWN = 0;
    ASCENDING = 1;
    DESCENDING = 2;
  }
  int64 dimension = 1;
  bool is_stable = 2;
  SortDirection direction = 3;  // TODO(basioli) optional
  string comparator_name = 4;
  repeated ShapeBufferAllocationSliceProto inputs_shapes = 5;
}

message XnnFusionThunkProto {}

message XnnDotThunkProto {
  DotDimensionNumbers dot_dimensions = 1;
  ShapeBufferAllocationSliceProto lhs_buffer_shape = 2;
  ShapeBufferAllocationSliceProto rhs_buffer_shape = 3;
  ShapeBufferAllocationSliceProto out_buffer_shape = 4;
}

message DotThunkProto {
  DotDimensionNumbers dot_dimensions = 1;
  ShapeBufferAllocationSliceProto lhs_buffer_shape = 2;
  ShapeBufferAllocationSliceProto rhs_buffer_shape = 3;
  ShapeBufferAllocationSliceProto out_buffer_shape = 4;
}

message RngGetAndUpdateStateThunkProto {
  int64 delta = 1;
  BufferAllocationSliceProto state_buffer = 2;
}

message TopKThunkProto {
  int64 batch_size = 1;
  int64 input_size = 2;
  int64 k = 3;
  BufferAllocationSliceProto values_buffer = 4;
  BufferAllocationSliceProto output_buffer = 5;
  BufferAllocationSliceProto indices_buffer = 6;
}

message WhileThunkProto {
  ThunkSequenceProto cond_sequence = 1;
  ThunkSequenceProto body_sequence = 2;
  int64 trip_count = 3;  // TODO(basioli) optional
  BufferAllocationSliceProto cond_buffer = 4;
}

message KernelThunkProto {
  message ThreadDim {
    int64 x = 1;
    int64 y = 2;
    int64 z = 3;
  }
  string kernel_name = 1;
  ThreadDim thread_dim = 2;
  // TODO(basioli) maybe optional? NOTE this is a set in C++
  repeated int64 invariant_arguments = 3;
  // TODO(basioli) maybe optional?
  int64 min_alignment = 4;
  repeated BufferAllocationSliceProto arguments_buffers = 5;
  repeated BufferAllocationSliceProto results_buffers = 6;
}

message CopyThunkProto {
  ShapeBufferAllocationSliceProto src_buffer_shape = 1;
  ShapeBufferAllocationSliceProto dst_buffer_shape = 2;
}

message FftThunkProto {
  bool is_multi_thread_eigen = 1;
  int32 fft_type = 2;
  repeated int64 fft_length = 3;
  ShapeBufferAllocationSliceProto input_buffer_shape = 4;
  ShapeBufferAllocationSliceProto output_buffer_shape = 5;
}

message InfeedThunkProto {
  message InfeedResource {
    // TODO(basioli) these are pointers in C++, maybe optional?
    ResourceProto consume_token = 1;
    ResourceProto produce_token = 2;
  }

  InfeedResource infeed_resources = 1;
  repeated ShapeBufferAllocationSliceProto infeed_buffers_shapes = 2;
}

message OutfeedThunkProto {
  message OutfeedResource {
    // TODO(basioli) these are pointers in C++, maybe optional?
    ResourceProto consume_token = 1;
    ResourceProto produce_token = 2;
  }

  OutfeedResource outfeed_resources = 1;
  repeated ShapeBufferAllocationSliceProto outfeed_buffers_shapes = 2;
}

message CustomCallThunkProto {
  message OpBuffers {
    repeated ShapeBufferAllocationSliceProto arguments_shapes = 1;
    repeated ShapeBufferAllocationSliceProto results_shapes = 2;
  }
  CustomCallApiVersion api_version = 1;
  string target_name = 2;
  string backend_config = 3;
  OpBuffers op_buffers = 4;
}

message InfoProto {
  string op_name = 1;
  string module_name = 2;
  int64 module_id = 3;
}

message ThunkProto {
  string kind = 1;
  InfoProto info = 2;
  oneof impl {
    CallThunkProto call_thunk = 3;
    ConditionalThunkProto conditional_thunk = 4;
    SortThunkProto sort_thunk = 5;
    XnnFusionThunkProto xnn_fusion_thunk = 6;
    XnnDotThunkProto xnn_dot_thunk = 7;
    DotThunkProto dot_thunk = 8;
    RngGetAndUpdateStateThunkProto rng_get_and_update_state_thunk = 9;
    TopKThunkProto top_k_thunk = 10;
    WhileThunkProto while_thunk = 11;
    KernelThunkProto kernel_thunk = 12;
    CopyThunkProto copy_thunk = 13;
    FftThunkProto fft_thunk = 14;
    InfeedThunkProto infeed_thunk = 15;
    OutfeedThunkProto outfeed_thunk = 16;
    CustomCallThunkProto custom_call_thunk = 17;
    ConvolutionThunkProto convolution_thunk = 18;
    CollectiveThunkProto collective_thunk = 19;
  }
}

message ThunkSequenceProto {
  repeated ThunkProto thunks = 1;
}
