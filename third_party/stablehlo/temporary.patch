diff --ruN a/stablehlo/build_tools/math/generate_ChloDecompositionPatternsMath.py b/stablehlo/build_tools/math/generate_ChloDecompositionPatternsMath.py
--- stablehlo/build_tools/math/generate_ChloDecompositionPatternsMath.py
+++ stablehlo/build_tools/math/generate_ChloDecompositionPatternsMath.py
@@ -71,8 +71,15 @@
 
   output_file = os.path.relpath(
       os.path.normpath(
-          os.path.join(os.path.dirname(__file__), "..", "..", "stablehlo",
-                       "transforms", output_filename)),
+          os.path.join(
+              os.path.dirname(__file__),
+              "..",
+              "..",
+              "stablehlo",
+              "transforms",
+              output_filename,
+          )
+      ),
       os.getcwd(),
   )
 
@@ -105,7 +112,8 @@
     func = getattr(fa.algorithms, fname, None)
     if func is None:
       warnings.warn(
-          f"{fa.algorithms.__name__} does not define {fname}. Skipping.")
+          f"{fa.algorithms.__name__} does not define {fname}. Skipping."
+      )
       continue
     ctx = fa.Context(paths=[fa.algorithms],
                      parameters=dict(rewrite_keep_integer_literals=True))
@@ -116,14 +124,15 @@
     sources[-1] += src
   source = "\n\n".join(sources) + "\n"
 
-  if chloname.startswith('StableHLO_'):
+  if chloname.startswith("StableHLO_"):
     # an ugly hack to fix the definition of stablehlo complex math
     # functions. TODO(pearu): add the corresponding feature to
     # functional_algorithms stablehlo printer
-    NameOp = chloname.split('_', 1)[1]
+    NameOp = chloname.split("_", 1)[1]
     source = source.replace(
-        f'def : Pat<({chloname}',
-        f'def {NameOp}_ComplexElementType_ComplexMathExpander : Pat<({chloname}'
+        f"def : Pat<({chloname}",
+        f"def {NameOp}_ComplexElementType_ComplexMathExpander :"
+        f" Pat<({chloname}",
     )
 
   if os.path.isfile(output_file):
diff --ruN a/stablehlo/build_tools/math/generate_tests.py b/stablehlo/build_tools/math/generate_tests.py
--- stablehlo/build_tools/math/generate_tests.py
+++ stablehlo/build_tools/math/generate_tests.py
@@ -64,10 +64,12 @@
     dict(name="acosh", mpmath_name="arccosh"),
     dict(name="atanh", mpmath_name="arctanh"),
     dict(name="square", mpmath_name="square"),
-    dict(name="log_plus_one",
-         mpmath_name="log1p",
-         namespace="stablehlo",
-         passes="--stablehlo-complex-math-expander"),
+    dict(
+        name="log_plus_one",
+        mpmath_name="log1p",
+        namespace="stablehlo",
+        passes="--stablehlo-complex-math-expander",
+    ),
 ]
 
 
@@ -138,13 +140,16 @@
       params = fa.utils.function_validation_parameters(opname, dtype)
       max_ulp_difference = op.get(
           "max_ulp_difference",
-          params.get("max_valid_ulp_count", default_max_ulp_difference))
+          params.get("max_valid_ulp_count", default_max_ulp_difference),
+      )
 
       nmp = fa.utils.numpy_with_mpmath(
           extra_prec_multiplier=op.get(
               "extra_prec_multiplier",
-              params.get("extra_prec_multiplier",
-                         default_extra_prec_multiplier)),
+              params.get(
+                  "extra_prec_multiplier", default_extra_prec_multiplier
+              ),
+          ),
           flush_subnormals=flush_subnormals,
       )
 
@@ -208,8 +213,10 @@
           continue
 
       f = open(fname, "w")
-      f.write(f"// RUN: stablehlo-opt {passes} %s |"
-              " stablehlo-translate --interpret\n")
+      f.write(
+          f"// RUN: stablehlo-opt {passes} %s |"
+          " stablehlo-translate --interpret\n"
+      )
       f.write(
           "// This file is generated, see build_tools/math/README.md for more"
           " information.\n")
diff --ruN a/stablehlo/docs/spec.md b/stablehlo/docs/spec.md
--- stablehlo/docs/spec.md
+++ stablehlo/docs/spec.md
@@ -5593,8 +5593,7 @@
 #### Constraints
 
 * (C1) `same(shape(inputs...))`.
-* (C2) `rank(inputs[0]) = size(update_window_dims) + size(inserted_window_dims)
-       + size(input_batching_dims)`.
+* (C2) `rank(inputs[0]) = size(update_window_dims) + size(inserted_window_dims) + size(input_batching_dims)`.
 * (C3) `same(shape(updates...))`.
 * (C4) `shape(updates[0]) = combine(update_scatter_dim_sizes,
        update_window_dim_sizes)` where:
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/TypeConversion.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/TypeConversion.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/TypeConversion.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/TypeConversion.cpp
@@ -107,6 +107,8 @@
 
 LinalgTypeConverter::LinalgTypeConverter() : RemoveSignTypeConverter() {
   addArgumentMaterialization(scalarToTensor);
+  addSourceMaterialization(scalarToTensor);
+  addTargetMaterialization(scalarToTensor);
 }
 
 }  // namespace mlir::stablehlo
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
@@ -440,7 +440,6 @@
 }
 
 // -----
-
 
 // CHECK-LABEL:   func.func @asinh_f64(
 // CHECK-SAME:                         %[[VAL_0:.*]]: tensor<f64>) -> tensor<f64> {
@@ -2788,7 +2787,6 @@
 
 // -----
 
-
 // CHECK-LABEL: @sinh_f32
 // CHECK-SAME: (%[[X:.*]]: tensor<f32>)
 func.func @sinh_f32(%x : tensor<f32>) -> tensor<f32> {
@@ -3891,6 +3889,8 @@
   return
 }
 
+// -----
+
 // CHECK-LABEL:   @square_complex_f32(
 // CHECK-SAME:                                  %[[VAL_0:.*]]: tensor<complex<f32>>) -> tensor<complex<f32>> {
 // CHECK:           %[[VAL_1:.*]] = stablehlo.real %[[VAL_0]] : (tensor<complex<f32>>) -> tensor<f32>
@@ -3916,6 +3916,8 @@
   func.return %result : tensor<complex<f32>>
 }
 
+// -----
+
 // CHECK-LABEL:   @square_f32(
 // CHECK-SAME:                          %[[VAL_0:.*]]: tensor<f32>) -> tensor<f32> {
 // CHECK:           %[[VAL_1:.*]] = stablehlo.multiply %[[VAL_0]], %[[VAL_0]] : tensor<f32>
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineArguments.cpp b/stablehlo/stablehlo/transforms/StablehloRefineArguments.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineArguments.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineArguments.cpp
@@ -72,78 +72,6 @@
     Type type = mlir::parseType(shape, context);
     if (!type) return module->emitOpError("Invalid type string: ") << shape;
     refinedTypes.push_back(type);
-  }
-  return success();
-}
-
-LogicalResult refinementError(func::FuncOp func, int64_t idx, Type argType,
-                              Type refinedType, StringRef msg) {
-  return func.emitOpError()
-         << "invalid refinement for argument " << idx << ", refinement " << msg
-         << " in " << mlir::debugString(argType) << " -> "
-         << mlir::debugString(refinedType);
-}
-
-// Validates refinement types:
-//   - A type refinement must be specified for each operand
-//   - Refinement types that match operand types are skipped
-//   - Refinement types that do not match operands must be refining tensors
-//   - Refined tensor types must be ranked, operand type can be unranked
-//   - Refined tensor types must match operand type for all static dimensions
-//
-LogicalResult validateRefinedTypes(func::FuncOp func, TypeRange refinedTypes) {
-  // Validate refined shapes
-  if (func.getNumArguments() != refinedTypes.size()) {
-    return func.emitOpError(
-               "number of refinements must match number of function operands ")
-           << refinedTypes.size() << " vs " << func.getNumArguments();
-  }
-
-  // Validate that refinements are valid
-  auto argTypes = func.getArgumentTypes();
-  for (int64_t i = 0; i < func.getNumArguments(); ++i) {
-    Type type = argTypes[i];
-    Type refinedType = refinedTypes[i];
-
-    // Always allow skipping refinement
-    if (type == refinedType) continue;
-
-    // If mismatched, must be tensor types
-    auto tensorType = dyn_cast<TensorType>(type);
-    auto refinedTensorType = dyn_cast<TensorType>(refinedType);
-    if (!tensorType || !refinedTensorType) {
-      return refinementError(func, i, type, refinedType, "must be a tensor");
-    }
-
-    // Check that element types match
-    if (tensorType.getElementType() != refinedTensorType.getElementType()) {
-      return refinementError(func, i, type, refinedType,
-                             "element types must match");
-    }
-
-    // Refined rank cannot be unranked if mismatch
-    if (isa<UnrankedTensorType>(refinedType)) {
-      return refinementError(func, i, type, refinedType, "must be ranked");
-    }
-
-    // Unranked operands can be refined to anything
-    if (!tensorType.hasRank()) continue;
-
-    // Validate ranks match if ranked (must allow unranked tensorType)
-    if (tensorType.getRank() != refinedTensorType.getRank()) {
-      return refinementError(func, i, type, refinedType,
-                             "rank must match operand rank");
-    }
-
-    // Validate static dimension sizes match
-    for (auto [dimSize, refinedDimSize] :
-         llvm::zip(tensorType.getShape(), refinedTensorType.getShape())) {
-      if (!ShapedType::isDynamic(dimSize) && dimSize != refinedDimSize) {
-        return refinementError(
-            func, i, type, refinedType,
-            "dimension sizes must match for static dimensions");
-      }
-    }
   }
   return success();
 }
@@ -219,9 +147,74 @@
 
 }  // namespace
 
+LogicalResult refinementError(Operation* op, int64_t idx, Type argType,
+                              Type refinedType, StringRef msg) {
+  return op->emitOpError()
+         << "invalid refinement for argument " << idx << ", refinement " << msg
+         << " in " << mlir::debugString(argType) << " -> "
+         << mlir::debugString(refinedType);
+}
+
+LogicalResult validateRefinedTypes(Operation* op, TypeRange argTypes, TypeRange refinedTypes) {
+  // Validate refined shapes
+  if (argTypes.size() != refinedTypes.size()) {
+    return op->emitOpError(
+               "number of refinements must match number of op operands ")
+           << refinedTypes.size() << " vs " << argTypes.size();
+  }
+
+  // Validate that refinements are valid
+  for (int64_t i = 0; i < argTypes.size(); ++i) {
+    Type type = argTypes[i];
+    Type refinedType = refinedTypes[i];
+
+    // Always allow skipping refinement
+    if (type == refinedType) continue;
+
+    // If mismatched, must be tensor types
+    auto tensorType = dyn_cast<TensorType>(type);
+    auto refinedTensorType = dyn_cast<TensorType>(refinedType);
+    if (!tensorType || !refinedTensorType) {
+      return refinementError(op, i, type, refinedType, "must be a tensor");
+    }
+
+    // Check that element types match
+    if (tensorType.getElementType() != refinedTensorType.getElementType()) {
+      return refinementError(op, i, type, refinedType,
+                             "element types must match");
+    }
+
+    // Refined rank cannot be unranked if mismatch
+    if (isa<UnrankedTensorType>(refinedType)) {
+      return refinementError(op, i, type, refinedType, "must be ranked");
+    }
+
+    // Unranked operands can be refined to anything
+    if (!tensorType.hasRank()) continue;
+
+    // Validate ranks match if ranked (must allow unranked tensorType)
+    if (tensorType.getRank() != refinedTensorType.getRank()) {
+      return refinementError(op, i, type, refinedType,
+                             "rank must match operand rank");
+    }
+
+    // Validate static dimension sizes match
+    for (auto [dimSize, refinedDimSize] :
+         llvm::zip(tensorType.getShape(), refinedTensorType.getShape())) {
+      if (!ShapedType::isDynamic(dimSize) && dimSize != refinedDimSize) {
+        return refinementError(
+            op, i, type, refinedType,
+            "dimension sizes must match for static dimensions");
+      }
+    }
+  }
+  return success();
+}
+
 LogicalResult refineArguments(func::FuncOp func, TypeRange refinedTypes) {
   // Verify that refinements are valid
-  if (failed(validateRefinedTypes(func, refinedTypes))) return failure();
+  if (failed(validateRefinedTypes(func, func.getArgumentTypes(), refinedTypes)))
+    return failure();
 
   // Wrap refined operands in operand wrapper to keep IR valid for refinement
   wrapRefinedOperands(func, refinedTypes);
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -109,7 +109,13 @@
       // their operands and results. Any operand type in these ops can change
       // within what's supported by `inferMostSpecificType` without breaking
       // verification of the op.
-      if (isa<chlo::ChloDialect, StablehloDialect>(user->getDialect()))
+      if (isa<chlo::ChloDialect, StablehloDialect>(
+              user->getDialect()))
+        continue;
+      // TODO(bartchr): Consider if the dialect allow-listing approach is too
+      // strict. In the meantime, allow some shape interop with the shardy
+      // dialect.
+      if (user->getDialect()->getNamespace() == "sdy")
         continue;
 
       // Simply changing operand type of `func.return` won't work because
@@ -1023,7 +1029,7 @@
   // upstream, and that might be the reason.
   config.useTopDownTraversal = true;
   config.enableRegionSimplification = GreedySimplifyRegionLevel::Aggressive;
-  config.maxIterations = 2;
+  config.maxIterations = 3;
   config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
   config.strictMode = GreedyRewriteStrictness::AnyOp;
 
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.h b/stablehlo/stablehlo/transforms/StablehloRefineShapes.h
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.h
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.h
@@ -16,18 +16,37 @@
 #ifndef STABLEHLO_TRANSFORMS_STABLEHLO_REFINE_SHAPES_H
 #define STABLEHLO_TRANSFORMS_STABLEHLO_REFINE_SHAPES_H
 
+#include <cstdint>
+
+#include "llvm/ADT/SmallVector.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/Operation.h"
 #include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeRange.h"
 #include "mlir/IR/Types.h"
 #include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
 #include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LLVM.h"
 #include "mlir/Support/LogicalResult.h"
 #include "stablehlo/dialect/Base.h"
 
 namespace mlir {
 namespace stablehlo {
+
+// Emits an error message for invalid refinement.
+LogicalResult refinementError(Operation* op, int64_t idx, Type argType,
+                              Type refinedType, StringRef msg);
+
+// Validates refinement types:
+//   - A type refinement must be specified for each operand
+//   - Refinement types that match operand types are skipped
+//   - Refinement types that do not match operands must be refining tensors
+//   - Refined tensor types must be ranked, operand type can be unranked
+//   - Refined tensor types must match operand type for all static dimensions
+LogicalResult validateRefinedTypes(Operation* op, TypeRange argTypes,
+                                   TypeRange refinedTypes);
 
 // Gets a FuncOp that --stablehlo-refine-shapes will run on.
 // Returns a nullptr and emits appropriate errors if such a function cannot

